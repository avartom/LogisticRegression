{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAD3Q3EHUVC6"
      },
      "source": [
        "BloomTech Data Science\n",
        "\n",
        "*Unit 2, Sprint 1, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoKAX4OyUVC-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nxYdlJpUVC_"
      },
      "source": [
        "# Module Project: Logistic Regression\n",
        "\n",
        "Do you like burritos? ðŸŒ¯ You're in luck then, because in this project you'll create a model to predict whether a burrito is `'Great'`.\n",
        "\n",
        "The dataset for this assignment comes from [Scott Cole](https://srcole.github.io/100burritos/), a San Diego-based data scientist and burrito enthusiast. \n",
        "\n",
        "## Directions\n",
        "\n",
        "The tasks for this project are the following:\n",
        "\n",
        "- **Task 1:** Import `csv` file using `wrangle` function.\n",
        "- **Task 2:** Conduct exploratory data analysis (EDA), and modify `wrangle` function .\n",
        "- **Task 3:** Split data into feature matrix `X` and target vector `y`.\n",
        "- **Task 4:** Split feature matrix `X` and target vector `y` into training and test sets.\n",
        "- **Task 5:** Establish the baseline accuracy score for your dataset.\n",
        "- **Task 6:** Build `model_logr` using a pipeline that includes three transfomers and `LogisticRegression` predictor. Train model on `X_train` and `X_test`.\n",
        "- **Task 7:** Calculate the training and test accuracy score for your model.\n",
        "- **Task 8:** Create a horizontal bar chart showing the 10 most influencial features for your  model. \n",
        "- **Task 9:** Demonstrate and explain the differences between `model_lr.predict()` and `model_lr.predict_proba()`.\n",
        "\n",
        "**Note** \n",
        "\n",
        "You should limit yourself to the following libraries:\n",
        "\n",
        "- `category_encoders`\n",
        "- `matplotlib`\n",
        "- `pandas`\n",
        "- `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzlJRZ5KUVDA"
      },
      "source": [
        "# I. Wrangle Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op3mPJuUUVDA"
      },
      "outputs": [],
      "source": [
        "from category_encoders import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def wrangle(filepath):\n",
        "    # Import w/ DateTimeIndex\n",
        "    df = pd.read_csv(filepath, parse_dates=['Date'],\n",
        "                     index_col='Date')\n",
        "    \n",
        "    # Drop unrated burritos\n",
        "    df.dropna(subset=['overall'], inplace=True)\n",
        "    \n",
        "    # Derive binary classification target:\n",
        "    # We define a 'Great' burrito as having an\n",
        "    # overall rating of 4 or higher, on a 5 point scale\n",
        "    df['Great'] = (df['overall'] >= 4).astype(int)\n",
        "    \n",
        "    # Drop high cardinality categoricals\n",
        "    df = df.drop(columns=['Notes', 'Location', 'Address', 'URL', 'Neighborhood'])\n",
        "    \n",
        "    # Drop columns to prevent \"leakage\"\n",
        "    df = df.drop(columns=['Rec', 'overall'])\n",
        "    \n",
        "    return df\n",
        "\n",
        "filepath = DATA_PATH + 'burritos/burritos.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSWxGRMoUVDB"
      },
      "source": [
        "**Task 1:** Use the above `wrangle` function to import the `burritos.csv` file into a DataFrame named `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tor69A0nUVDC"
      },
      "outputs": [],
      "source": [
        "filepath = DATA_PATH + 'burritos/burritos.csv'\n",
        "df = wrangle(filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9ogALSjUVDC"
      },
      "source": [
        "During your exploratory data analysis, note that there are several columns whose data type is `object` but that seem to be a binary encoding. For example, `df['Beef'].head()` returns:\n",
        "\n",
        "```\n",
        "0      x\n",
        "1      x\n",
        "2    NaN\n",
        "3      x\n",
        "4      x\n",
        "Name: Beef, dtype: object\n",
        "```\n",
        "\n",
        "**Task 2:** Change the `wrangle` function so that these columns are properly encoded as `0` and `1`s. Be sure your code handles upper- and lowercase `X`s, and `NaN`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxY8gE5jUVDD"
      },
      "outputs": [],
      "source": [
        "# Conduct your exploratory data analysis here\n",
        "# And modify the `wrangle` function above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrQp9oUQUVDD"
      },
      "source": [
        "If you explore the `'Burrito'` column of `df`, you'll notice that it's a high-cardinality categorical feature. You'll also notice that there's a lot of overlap between the categories. \n",
        "\n",
        "**Stretch Goal:** Change the `wrangle` function above so that it engineers four new features: `'california'`, `'asada'`, `'surf'`, and `'carnitas'`. Each row should have a `1` or `0` based on the text information in the `'Burrito'` column. For example, here's how the first 5 rows of the dataset would look.\n",
        "\n",
        "| **Burrito** | **california** | **asada** | **surf** | **carnitas** |\n",
        "| :---------- | :------------: | :-------: | :------: | :----------: |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "|  Carnitas   |       0        |     0     |    0     |      1       |\n",
        "| Carne asada |       0        |     1     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "\n",
        "**Note:** Be sure to also drop the `'Burrito'` once you've engineered your new features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e33QsVEcUVDE"
      },
      "outputs": [],
      "source": [
        "# Conduct your exploratory data analysis here\n",
        "# And modify the `wrangle` function above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aofkYwv6UVDE"
      },
      "source": [
        "# II. Split Data\n",
        "\n",
        "**Task 3:** Split your dataset into the feature matrix `X` and the target vector `y`. You want to predict `'Great'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o7b-knVUVDF",
        "outputId": "de3a8b17-4ebe-4be0-bc27-bdd8039e335c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    239\n",
              "1    182\n",
              "Name: Great, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 356
        }
      ],
      "source": [
        "target = \"Great\"\n",
        "X = df.drop(columns = target)\n",
        "y = df[target]\n",
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBdK8SOaUVDF"
      },
      "source": [
        "**Task 4:** Split `X` and `y` into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`).\n",
        "\n",
        "- Your training set should include data from 2016 through 2017. \n",
        "- Your test set should include data from 2018 and later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWTTytuzUVDF"
      },
      "outputs": [],
      "source": [
        "cutoff = '2018'\n",
        "mask = X.index < cutoff\n",
        "X_train, y_train = X.loc[mask], y.loc[mask]\n",
        "X_test, y_test = X[~mask], y[~mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNHto_x7UVDF"
      },
      "source": [
        "# III. Establish Baseline\n",
        "\n",
        "**Task 5:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1lmeqxmUVDG",
        "outputId": "24149351-5cca-4ad6-93b5-3953974281a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy Score: 0.5822454308093995\n"
          ]
        }
      ],
      "source": [
        "baseline_acc = y_train.value_counts(normalize=True).max()\n",
        "print('Baseline Accuracy Score:', baseline_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPHpvOwOUVDG"
      },
      "source": [
        "# IV. Build Model\n",
        "\n",
        "**Task 6:** Build a `Pipeline` named `model_logr`, and fit it to your training data. Your pipeline should include:\n",
        "\n",
        "- a `OneHotEncoder` transformer for categorical features, \n",
        "- a `SimpleImputer` transformer to deal with missing values, \n",
        "- a [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transfomer (which often improves performance in a logistic regression model), and \n",
        "- a `LogisticRegression` predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNiNR6ftUVDG",
        "outputId": "b4051904-1d56-4c83-988d-8f276d61ef5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('onehotencoder',\n",
              "                 OneHotEncoder(cols=['Burrito', 'Chips', 'Reviewer',\n",
              "                                     'Unreliable', 'NonSD', 'Beef', 'Pico',\n",
              "                                     'Guac', 'Cheese', 'Fries', 'Sour cream',\n",
              "                                     'Pork', 'Chicken', 'Shrimp', 'Fish',\n",
              "                                     'Rice', 'Beans', 'Lettuce', 'Tomato',\n",
              "                                     'Bell peper', 'Carrots', 'Cabbage',\n",
              "                                     'Sauce', 'Salsa.1', 'Cilantro', 'Onion',\n",
              "                                     'Taquito', 'Pineapple', 'Ham',\n",
              "                                     'Chile relleno', ...],\n",
              "                               use_cat_names=True)),\n",
              "                ('simpleimputer', SimpleImputer()),\n",
              "                ('standardscaler', StandardScaler()),\n",
              "                ('logisticregression', LogisticRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 360
        }
      ],
      "source": [
        "model_logr = make_pipeline(OneHotEncoder(use_cat_names= True), SimpleImputer(), StandardScaler(), LogisticRegression())\n",
        "model_logr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JTizaniUVDG"
      },
      "source": [
        "# IV. Check Metrics\n",
        "\n",
        "**Task 7:** Calculate the training and test accuracy score for `model_lr`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CVEw892UVDH",
        "outputId": "5b7cd957-e23b-48aa-b3e8-d01a46218c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MAE: 0.9921671018276762\n",
            "Test MAE: 0.7631578947368421\n"
          ]
        }
      ],
      "source": [
        "training_acc = model_logr.score(X_train, y_train)\n",
        "test_acc = model_logr.score(X_test, y_test)\n",
        "\n",
        "print('Training MAE:', training_acc)\n",
        "print('Test MAE:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6TeOvSZUVDH"
      },
      "source": [
        "# V. Communicate Results\n",
        "\n",
        "**Task 8:** Create a horizontal barchart that plots the 10 most important coefficients for `model_lr`, sorted by absolute value.\n",
        "\n",
        "**Note:** Since you created your model using a `Pipeline`, you'll need to use the [`named_steps`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) attribute to access the coefficients in your `LogisticRegression` predictor. Be sure to look at the shape of the coefficients array before you combine it with the feature names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "GPjiEUXBUVDH",
        "outputId": "3c984239-ca65-49fb-ff47-7eba3c49136a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f998a9ebd10>"
            ]
          },
          "metadata": {},
          "execution_count": 374
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Create your horizontal barchart here.\n",
        "coefficients = model_logr.named_steps['logisticregression'].coef_[0]\n",
        "features = model_logr.named_steps['onehotencoder'].get_feature_names()\n",
        "feat_imp = pd.Series(coefficients).sort_values(key = abs)\n",
        "feat_imp.tail(10).plot(kind = 'barh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "her-Yt3HUVDH"
      },
      "source": [
        "There is more than one way to generate predictions with `model_lr`. For instance, you can use [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression) or [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression.predict_proba).\n",
        "\n",
        "**Task 9:** Generate predictions for `X_test` using both `predict` and `predict_proba`. Then below, write a summary of the differences in the output for these two methods. You should answer the following questions:\n",
        "\n",
        "- What data type do `predict` and `predict_proba` output?\n",
        "- What are the shapes of their different output?\n",
        "- What numerical values are in the output?\n",
        "- What do those numerical values represent?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umuFK8UzUVDI",
        "outputId": "e3ba6aa0-0ef3-4b2c-d323-3d96b06910b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.62567303e-05, 9.99923743e-01],\n",
              "       [2.42692147e-03, 9.97573079e-01],\n",
              "       [9.42426184e-01, 5.75738163e-02],\n",
              "       [1.82516292e-05, 9.99981748e-01],\n",
              "       [9.85488813e-01, 1.45111866e-02],\n",
              "       [9.69847316e-01, 3.01526836e-02],\n",
              "       [1.62745237e-02, 9.83725476e-01],\n",
              "       [1.48222683e-04, 9.99851777e-01],\n",
              "       [1.50948915e-04, 9.99849051e-01],\n",
              "       [1.44078646e-01, 8.55921354e-01],\n",
              "       [6.70784716e-01, 3.29215284e-01],\n",
              "       [7.36027316e-01, 2.63972684e-01],\n",
              "       [8.92516068e-01, 1.07483932e-01],\n",
              "       [2.62893246e-01, 7.37106754e-01],\n",
              "       [9.15863225e-03, 9.90841368e-01],\n",
              "       [6.69840878e-02, 9.33015912e-01],\n",
              "       [3.19332885e-01, 6.80667115e-01],\n",
              "       [9.97212163e-01, 2.78783710e-03],\n",
              "       [9.93332162e-01, 6.66783833e-03],\n",
              "       [9.98933598e-01, 1.06640154e-03],\n",
              "       [9.92524470e-01, 7.47553009e-03],\n",
              "       [7.15122571e-03, 9.92848774e-01],\n",
              "       [9.43083700e-01, 5.69163000e-02],\n",
              "       [1.43986770e-02, 9.85601323e-01],\n",
              "       [9.42779015e-01, 5.72209849e-02],\n",
              "       [9.90226164e-01, 9.77383630e-03],\n",
              "       [9.90544713e-01, 9.45528660e-03],\n",
              "       [1.71389979e-05, 9.99982861e-01],\n",
              "       [1.01406694e-01, 8.98593306e-01],\n",
              "       [1.96384390e-03, 9.98036156e-01],\n",
              "       [9.96452053e-01, 3.54794702e-03],\n",
              "       [4.76395869e-02, 9.52360413e-01],\n",
              "       [3.33318746e-03, 9.96666813e-01],\n",
              "       [9.90177270e-01, 9.82272983e-03],\n",
              "       [9.21397430e-01, 7.86025697e-02],\n",
              "       [1.14387200e-01, 8.85612800e-01],\n",
              "       [7.84042450e-05, 9.99921596e-01],\n",
              "       [6.54595457e-03, 9.93454045e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 376
        }
      ],
      "source": [
        "# Write code here to explore the differences between `predict` and `predict_proba`.\n",
        "\n",
        "model_logr.predict(X_test)\n",
        "model_logr.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-vbNGdRUVDI"
      },
      "source": [
        "*one gives the category the other one gives the probability of belonging to the respective category*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}